23/05/06 17:12:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
23/05/06 17:12:47 INFO SparkContext: Running Spark version 3.2.1
23/05/06 17:12:47 INFO ResourceUtils: ==============================================================
23/05/06 17:12:47 INFO ResourceUtils: No custom resources configured for spark.driver.
23/05/06 17:12:47 INFO ResourceUtils: ==============================================================
23/05/06 17:12:47 INFO SparkContext: Submitted application: com._4paradigm.openmldb.batchjob.ImportOfflineData
23/05/06 17:12:47 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/05/06 17:12:47 INFO ResourceProfile: Limiting resource is cpu
23/05/06 17:12:47 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/05/06 17:12:48 INFO SecurityManager: Changing view acls groups to: 
23/05/06 17:12:48 INFO SecurityManager: Changing modify acls groups to: 
23/05/06 17:12:48 INFO Utils: Successfully started service 'sparkDriver' on port 36555.
23/05/06 17:12:48 INFO SparkEnv: Registering MapOutputTracker
23/05/06 17:12:48 INFO SparkEnv: Registering BlockManagerMaster
23/05/06 17:12:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/05/06 17:12:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/05/06 17:12:48 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/05/06 17:12:48 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f993d1b1-ee87-434d-896c-bf248cf0ad79
23/05/06 17:12:48 INFO MemoryStore: MemoryStore started with capacity 4.1 GiB
23/05/06 17:12:48 INFO SparkEnv: Registering OutputCommitCoordinator
23/05/06 17:12:48 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/05/06 17:12:48 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
23/05/06 17:12:48 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
23/05/06 17:12:49 INFO Utils: Successfully started service 'SparkUI' on port 4043.
<<<<<<< HEAD
=======
23/05/06 17:12:49 INFO SparkContext: Added JAR file:/home/huangwei/openmldb-playground/openmldb-0.7.3-linux/taskmanager/lib/openmldb-batchjob-0.7.3.jar at spark://m7-pce-gpu02:36555/jars/openmldb-batchjob-0.7.3.jar with timestamp 1683364367844
>>>>>>> f00aff11a7679ae9c902c06aa4296302ecc31792
23/05/06 17:12:49 INFO SparkContext: Added file file:///tmp/sql-8121155936629523472 at file:///tmp/sql-8121155936629523472 with timestamp 1683364367844
23/05/06 17:12:49 INFO Utils: Copying /tmp/sql-8121155936629523472 to /tmp/spark-9a6c1e29-946a-4a37-9ba5-9574f4f08dd7/userFiles-6f11e5af-e5ee-4197-85ca-dcd02a6558aa/sql-8121155936629523472
23/05/06 17:12:49 INFO Executor: Fetching file:///tmp/sql-8121155936629523472 with timestamp 1683364367844
23/05/06 17:12:49 INFO Utils: /tmp/sql-8121155936629523472 has been previously copied to /tmp/spark-9a6c1e29-946a-4a37-9ba5-9574f4f08dd7/userFiles-6f11e5af-e5ee-4197-85ca-dcd02a6558aa/sql-8121155936629523472
23/05/06 17:12:49 INFO Executor: Adding file:/tmp/spark-9a6c1e29-946a-4a37-9ba5-9574f4f08dd7/userFiles-6f11e5af-e5ee-4197-85ca-dcd02a6558aa/openmldb-batchjob-0.7.3.jar to class loader
23/05/06 17:12:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42168.
23/05/06 17:12:49 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/05/06 17:12:50 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/05/06 17:12:52 INFO ConfigReflections$: Native Spark Configuration: spark.sql.session.timeZone -> Asia/Shanghai
23/05/06 17:12:52 INFO ConfigReflections$: Native Spark Configuration: openmldb.zk.cluster -> 127.0.0.1:8181
23/05/06 17:12:52 INFO ConfigReflections$: Native Spark Configuration: openmldb.default.db -> db
23/05/06 17:12:52 INFO LibraryLoader: Can not find libsql_jsdk.so from environment, try find in resources
23/05/06 17:12:52 INFO LibraryLoader: Found libsql_jsdk.so in local resource
23/05/06 17:12:59 INFO LibraryLoader: Extract resource to /tmp/temp-2806441537945956809libsql_jsdk.so
2023-05-06 17:13:00,054:26788(0x7fe45cd97700):ZOO_INFO@log_env@753: Client environment:zookeeper.version=zookeeper C client 3.4.14
2023-05-06 17:13:00,054:26788(0x7fe45cd97700):ZOO_INFO@log_env@764: Client environment:os.name=Linux
2023-05-06 17:13:00,054:26788(0x7fe45cd97700):ZOO_INFO@log_env@765: Client environment:os.arch=3.10.0-1160.6.1.el7.x86_64
2023-05-06 17:13:00,054:26788(0x7fe45cd97700):ZOO_INFO@log_env@766: Client environment:os.version=#1 SMP Tue Nov 17 13:59:11 UTC 2020
2023-05-06 17:13:00,054:26788(0x7fe45cd97700):ZOO_INFO@log_env@774: Client environment:user.name=(null)
2023-05-06 17:13:00,054:26788(0x7fe45cd97700):ZOO_INFO@zookeeper_init@827: Initiating client connection, host=127.0.0.1:8181 sessionTimeout=10000 watcher=0x7fe26bd21e00 sessionId=0 sessionPasswd=<null> context=0x7fe4553115b0 flags=0
2023-05-06 17:13:00,055:26788(0x7fe3c6bfb700):ZOO_INFO@check_events@1764: initiated connection to server [127.0.0.1:8181]
2023-05-06 17:13:00,056:26788(0x7fe3c6bfb700):ZOO_INFO@check_events@1811: session establishment complete on server [127.0.0.1:8181], sessionId=0x1025071ec790037, negotiated timeout=10000
I0506 17:13:00.056802 27445 zk_client.cc:564] zookeeper event with type -1, state 3, path 
I0506 17:13:00.057067 27445 zk_client.cc:579] connect success
I0506 17:13:00.063773 27018 client_manager.cc:483] add client. name 172.24.4.27:7126, endpoint 
I0506 17:13:00.063843 27018 client_manager.cc:483] add client. name 172.24.4.27:7128, endpoint 
I0506 17:13:00.066839 27018 db_sdk.cc:234] start to watch notify on table, function, ns leader, taskamanger leader
I0506 17:13:00.068267 27018 db_sdk.cc:55] init ns client with endpoint 172.24.4.27:7123 done
I0506 17:13:00.078817 27018 default_udf_library.cc:49] Creating DefaultUdfLibrary
23/05/06 17:13:00 INFO ZooKeeper: Client environment:zookeeper.version=3.6.2--803c7f1a12f85978cb049af5e4ef23bd8b688715, built on 09/04/2020 12:44 GMT
23/05/06 17:13:00 INFO ZooKeeper: Client environment:java.version=1.8.0_302
23/05/06 17:13:00 INFO ZooKeeper: Client environment:java.vendor=Red Hat, Inc.
23/05/06 17:13:00 INFO ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.302.b08-0.el7_9.x86_64/jre
23/05/06 17:13:00 INFO ZooKeeper: Client environment:java.io.tmpdir=/tmp
23/05/06 17:13:00 INFO ZooKeeper: Client environment:java.compiler=<NA>
23/05/06 17:13:00 INFO ZooKeeper: Client environment:os.name=Linux
23/05/06 17:13:00 INFO ZooKeeper: Client environment:os.arch=amd64
23/05/06 17:13:00 INFO ZooKeeper: Client environment:os.version=3.10.0-1160.6.1.el7.x86_64
23/05/06 17:13:00 INFO ZooKeeper: Client environment:os.memory.free=2693MB
23/05/06 17:13:00 INFO ZooKeeper: Client environment:os.memory.max=7282MB
23/05/06 17:13:00 INFO ZooKeeper: Client environment:os.memory.total=3026MB
23/05/06 17:13:00 INFO CuratorFrameworkImpl: Starting
23/05/06 17:13:00 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:8181 sessionTimeout=5000 watcher=org.apache.curator.ConnectionState@1373e3ee
23/05/06 17:13:00 INFO X509Util: Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
23/05/06 17:13:00 INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes
23/05/06 17:13:00 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false
23/05/06 17:13:00 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:8181.
23/05/06 17:13:00 INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)
23/05/06 17:13:00 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:60688, server: localhost/127.0.0.1:8181
23/05/06 17:13:00 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:8181, session id = 0x1025071ec79003a, negotiated timeout = 5000
23/05/06 17:13:00 INFO CuratorFrameworkImpl: Default schema
23/05/06 17:13:00 INFO ConnectionStateManager: State change: CONNECTED
23/05/06 17:13:00 INFO OpenmldbSession: Register empty dataframe of db.e with schema StructType(StructField(c1,IntegerType,true), StructField(c2,IntegerType,true))
23/05/06 17:13:02 INFO OpenmldbSession: Register empty dataframe of demo_db.demo_table1 with schema StructType(StructField(c1,StringType,true), StructField(c2,IntegerType,true), StructField(c3,LongType,true), StructField(c4,FloatType,true), StructField(c5,DoubleType,true), StructField(c6,TimestampType,true), StructField(c7,DateType,true))
23/05/06 17:13:02 INFO OpenmldbSession: Register empty dataframe of disk_test.fulltable with schema StructType(StructField(c1,StringType,true), StructField(c2,ShortType,true), StructField(c3,IntegerType,true), StructField(c4,LongType,true), StructField(c5,FloatType,true), StructField(c6,DoubleType,true), StructField(c7,StringType,true), StructField(c8,DateType,true), StructField(c9,TimestampType,true), StructField(c10,StringType,true))
23/05/06 17:13:02 INFO ConfigReflections$: Native Spark Configuration: spark.sql.session.timeZone -> Asia/Shanghai
23/05/06 17:13:02 INFO ConfigReflections$: Native Spark Configuration: openmldb.zk.cluster -> 127.0.0.1:8181
23/05/06 17:13:02 INFO ConfigReflections$: Native Spark Configuration: openmldb.default.db -> db
2023-05-06 17:13:02,528:26788(0x7fe45cd97700):ZOO_INFO@log_env@753: Client environment:zookeeper.version=zookeeper C client 3.4.14
2023-05-06 17:13:02,528:26788(0x7fe45cd97700):ZOO_INFO@log_env@764: Client environment:os.name=Linux
2023-05-06 17:13:02,528:26788(0x7fe45cd97700):ZOO_INFO@log_env@765: Client environment:os.arch=3.10.0-1160.6.1.el7.x86_64
2023-05-06 17:13:02,528:26788(0x7fe45cd97700):ZOO_INFO@log_env@766: Client environment:os.version=#1 SMP Tue Nov 17 13:59:11 UTC 2020
2023-05-06 17:13:02,528:26788(0x7fe45cd97700):ZOO_INFO@log_env@774: Client environment:user.name=(null)
2023-05-06 17:13:02,528:26788(0x7fe45cd97700):ZOO_INFO@zookeeper_init@827: Initiating client connection, host=127.0.0.1:8181 sessionTimeout=10000 watcher=0x7fe26bd21e00 sessionId=0 sessionPasswd=<null> context=0x7fe22d069620 flags=0
2023-05-06 17:13:02,529:26788(0x7fe250bf9700):ZOO_INFO@check_events@1764: initiated connection to server [127.0.0.1:8181]
2023-05-06 17:13:02,530:26788(0x7fe250bf9700):ZOO_INFO@check_events@1811: session establishment complete on server [127.0.0.1:8181], sessionId=0x1025071ec79003d, negotiated timeout=10000
I0506 17:13:02.530835 27538 zk_client.cc:564] zookeeper event with type -1, state 3, path 
I0506 17:13:02.530972 27538 zk_client.cc:579] connect success
I0506 17:13:02.532748 27018 client_manager.cc:483] add client. name 172.24.4.27:7126, endpoint 
I0506 17:13:02.532783 27018 client_manager.cc:483] add client. name 172.24.4.27:7128, endpoint 
I0506 17:13:02.535025 27018 db_sdk.cc:234] start to watch notify on table, function, ns leader, taskamanger leader
I0506 17:13:02.536502 27018 db_sdk.cc:55] init ns client with endpoint 172.24.4.27:7123 done
23/05/06 17:13:02 INFO CuratorFrameworkImpl: Starting
23/05/06 17:13:02 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:8181 sessionTimeout=5000 watcher=org.apache.curator.ConnectionState@35524549
23/05/06 17:13:02 INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes
23/05/06 17:13:02 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false
23/05/06 17:13:02 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:8181.
23/05/06 17:13:02 INFO CuratorFrameworkImpl: Default schema
23/05/06 17:13:02 INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)
23/05/06 17:13:02 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:60716, server: localhost/127.0.0.1:8181
23/05/06 17:13:02 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:8181, session id = 0x1025071ec79003e, negotiated timeout = 5000
23/05/06 17:13:02 INFO ConnectionStateManager: State change: CONNECTED
23/05/06 17:13:02 INFO OpenmldbSession: Register empty dataframe of db.e with schema StructType(StructField(c1,IntegerType,true), StructField(c2,IntegerType,true))
23/05/06 17:13:02 INFO OpenmldbSession: Register empty dataframe of demo_db.demo_table1 with schema StructType(StructField(c1,StringType,true), StructField(c2,IntegerType,true), StructField(c3,LongType,true), StructField(c4,FloatType,true), StructField(c5,DoubleType,true), StructField(c6,TimestampType,true), StructField(c7,DateType,true))
23/05/06 17:13:02 INFO OpenmldbSession: Register empty dataframe of disk_test.fulltable with schema StructType(StructField(c1,StringType,true), StructField(c2,ShortType,true), StructField(c3,IntegerType,true), StructField(c4,LongType,true), StructField(c5,FloatType,true), StructField(c6,DoubleType,true), StructField(c7,StringType,true), StructField(c8,DateType,true), StructField(c9,TimestampType,true), StructField(c10,StringType,true))
23/05/06 17:13:02 INFO SparkPlanner: Disable window parallelization optimization, enable by setting openmldb.window.parallelization
23/05/06 17:13:02 INFO SqlEngine: Dumped module size: 103
23/05/06 17:13:02 INFO SparkPlanner: Visit physical plan to find ConcatJoin node
23/05/06 17:13:02 INFO SparkPlanner: Visit concat join node to add node index info
23/05/06 17:13:02 INFO LoadDataPlan$: table info: name: "e"
table_partition {
  pid: 0
  partition_meta {
    endpoint: "172.24.4.27:7126"
    is_leader: true
  }
  partition_meta {
    endpoint: "172.24.4.27:7128"
    is_leader: false
  }
  term_offset {
    term: 1
    offset: 0
  }
}
table_partition {
  pid: 1
  partition_meta {
    endpoint: "172.24.4.27:7126"
    is_leader: false
  }
  partition_meta {
    endpoint: "172.24.4.27:7128"
    is_leader: true
  }
  term_offset {
    term: 1
    offset: 0
  }
}
table_partition {
  pid: 2
  partition_meta {
    endpoint: "172.24.4.27:7126"
    is_leader: true
  }
  partition_meta {
    endpoint: "172.24.4.27:7128"
    is_leader: false
  }
  term_offset {
    term: 1
    offset: 0
  }
}
table_partition {
  pid: 3
  partition_meta {
    endpoint: "172.24.4.27:7126"
    is_leader: false
  }
  partition_meta {
    endpoint: "172.24.4.27:7128"
    is_leader: true
  }
  term_offset {
    term: 1
    offset: 0
  }
}
table_partition {
  pid: 4
  partition_meta {
    endpoint: "172.24.4.27:7126"
    is_leader: true
  }
  partition_meta {
    endpoint: "172.24.4.27:7128"
    is_leader: false
  }
  term_offset {
    term: 1
    offset: 0
  }
}
table_partition {
  pid: 5
  partition_meta {
    endpoint: "172.24.4.27:7126"
    is_leader: false
  }
  partition_meta {
    endpoint: "172.24.4.27:7128"
    is_leader: true
  }
  term_offset {
    term: 1
    offset: 0
  }
}
table_partition {
  pid: 6
  partition_meta {
    endpoint: "172.24.4.27:7126"
    is_leader: true
  }
  partition_meta {
    endpoint: "172.24.4.27:7128"
    is_leader: false
  }
  term_offset {
    term: 1
    offset: 0
  }
}
table_partition {
  pid: 7
  partition_meta {
    endpoint: "172.24.4.27:7126"
    is_leader: false
  }
  partition_meta {
    endpoint: "172.24.4.27:7128"
    is_leader: true
  }
  term_offset {
    term: 1
    offset: 0
  }
}
tid: 8
partition_num: 8
replica_num: 2
column_desc {
  name: "c1"
  data_type: kInt
  not_null: false
}
column_desc {
  name: "c2"
  data_type: kInt
  not_null: false
}
column_key {
  index_name: "INDEX_0_1683364365"
  col_name: "c1"
  ttl {
  }
}
db: "db"
storage_mode: kMemory

23/05/06 17:13:02 INFO HybridseUtil$: load data from hive table hive://a.b
23/05/06 17:13:03 INFO HiveConf: Found configuration file null
23/05/06 17:13:03 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
23/05/06 17:13:03 INFO HiveConf: Found configuration file null
23/05/06 17:13:05 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
23/05/06 17:13:05 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
23/05/06 17:13:05 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
23/05/06 17:13:05 INFO ObjectStore: ObjectStore, initialize called
23/05/06 17:13:05 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
23/05/06 17:13:05 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
23/05/06 17:13:07 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
23/05/06 17:13:09 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
23/05/06 17:13:09 INFO ObjectStore: Initialized ObjectStore
23/05/06 17:13:09 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
23/05/06 17:13:09 INFO HiveMetaStore: Added admin role in metastore
23/05/06 17:13:09 INFO HiveMetaStore: Added public role in metastore
23/05/06 17:13:09 INFO HiveMetaStore: No user is added in admin role, since config is empty
23/05/06 17:13:09 INFO HiveMetaStore: 0: get_database: default
23/05/06 17:13:09 INFO HiveMetaStore: 0: get_database: global_temp
23/05/06 17:13:09 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
23/05/06 17:13:09 INFO HiveMetaStore: 0: get_database: a
23/05/06 17:13:09 WARN ObjectStore: Failed to get database a, returning NoSuchObjectException
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com._4paradigm.openmldb.batch.api.OpenmldbSession.sparksql(OpenmldbSession.scala:209)
	at com._4paradigm.openmldb.batch.utils.HybridseUtil$.hiveLoad(HybridseUtil.scala:378)
	at com._4paradigm.openmldb.batch.utils.HybridseUtil$.autoLoad(HybridseUtil.scala:305)
	at com._4paradigm.openmldb.batch.nodes.LoadDataPlan$.gen(LoadDataPlan.scala:50)
	at com._4paradigm.openmldb.batch.SparkPlanner.visitNode(SparkPlanner.scala:269)
	at com._4paradigm.openmldb.batch.SparkPlanner.getSparkOutput(SparkPlanner.scala:232)
	at com._4paradigm.openmldb.batch.SparkPlanner.$anonfun$plan$1(SparkPlanner.scala:113)
	at com._4paradigm.openmldb.batch.SparkPlanner.withSQLEngine(SparkPlanner.scala:384)
	at com._4paradigm.openmldb.batch.SparkPlanner.plan(SparkPlanner.scala:83)
	at com._4paradigm.openmldb.batch.api.OpenmldbSession.openmldbSql(OpenmldbSession.scala:178)
	at com._4paradigm.openmldb.batch.api.OpenmldbSession.sql(OpenmldbSession.scala:189)
	at com._4paradigm.openmldb.batchjob.util.OpenmldbJobUtil$.runOpenmldbSql(OpenmldbJobUtil.scala:54)
	at com._4paradigm.openmldb.batchjob.ImportOfflineData$.importOfflineData(ImportOfflineData.scala:35)
	at com._4paradigm.openmldb.batchjob.ImportOfflineData$.main(ImportOfflineData.scala:27)
	at com._4paradigm.openmldb.batchjob.ImportOfflineData.main(ImportOfflineData.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: a.b; line 1 pos 14;
'Project [*]
+- 'UnresolvedRelation [a, b], [], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:123)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:263)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:262)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:262)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:94)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:205)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:202)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:88)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:196)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:196)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:88)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:86)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:78)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
	at org.apache.spark.sql.Dataset.ofRows(Dataset.scala)
	... 31 more
23/05/06 17:13:09 INFO SqlEngine: Dumped module size: 103
23/05/06 17:13:09 INFO SparkPlanner: Visit physical plan to find ConcatJoin node
23/05/06 17:13:09 INFO SparkPlanner: Visit concat join node to add node index info
23/05/06 17:13:09 INFO LoadDataPlan$: table info: name: "e"
table_partition {
  pid: 0
  partition_meta {
    endpoint: "172.24.4.27:7126"
    is_leader: true
  }
  partition_meta {
    endpoint: "172.24.4.27:7128"
    is_leader: false
  }
  term_offset {
    term: 1
    offset: 0
  }
}
table_partition {
  pid: 1
  partition_meta {
    endpoint: "172.24.4.27:7126"
    is_leader: false
  }
  partition_meta {
    endpoint: "172.24.4.27:7128"
    is_leader: true
  }
  term_offset {
    term: 1
    offset: 0
  }
}
table_partition {
  pid: 2
  partition_meta {
    endpoint: "172.24.4.27:7126"
    is_leader: true
  }
  partition_meta {
    endpoint: "172.24.4.27:7128"
    is_leader: false
  }
  term_offset {
    term: 1
    offset: 0
  }
}
table_partition {
  pid: 3
  partition_meta {
    endpoint: "172.24.4.27:7126"
    is_leader: false
  }
  partition_meta {
    endpoint: "172.24.4.27:7128"
    is_leader: true
  }
  term_offset {
    term: 1
    offset: 0
  }
}
table_partition {
  pid: 4
  partition_meta {
    endpoint: "172.24.4.27:7126"
    is_leader: true
  }
  partition_meta {
    endpoint: "172.24.4.27:7128"
    is_leader: false
  }
  term_offset {
    term: 1
    offset: 0
  }
}
table_partition {
  pid: 5
  partition_meta {
    endpoint: "172.24.4.27:7126"
    is_leader: false
  }
  partition_meta {
    endpoint: "172.24.4.27:7128"
    is_leader: true
  }
  term_offset {
    term: 1
    offset: 0
  }
}
table_partition {
  pid: 6
  partition_meta {
    endpoint: "172.24.4.27:7126"
    is_leader: true
  }
  partition_meta {
    endpoint: "172.24.4.27:7128"
    is_leader: false
  }
  term_offset {
    term: 1
    offset: 0
  }
}
table_partition {
  pid: 7
  partition_meta {
    endpoint: "172.24.4.27:7126"
    is_leader: false
  }
  partition_meta {
    endpoint: "172.24.4.27:7128"
    is_leader: true
  }
  term_offset {
    term: 1
    offset: 0
  }
}
tid: 8
partition_num: 8
replica_num: 2
column_desc {
  name: "c1"
  data_type: kInt
  not_null: false
}
column_desc {
  name: "c2"
  data_type: kInt
  not_null: false
}
column_key {
  index_name: "INDEX_0_1683364365"
  col_name: "c1"
  ttl {
  }
}
db: "db"
storage_mode: kMemory

23/05/06 17:13:09 INFO HybridseUtil$: load data from hive table hive://a.b
23/05/06 17:13:10 INFO HiveMetaStore: 0: get_database: a
23/05/06 17:13:10 WARN ObjectStore: Failed to get database a, returning NoSuchObjectException
Exception in thread "main" org.apache.spark.sql.AnalysisException: Table or view not found: a.b; line 1 pos 14;
'Project [*]
+- 'UnresolvedRelation [a, b], [], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:123)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:263)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:262)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:262)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:262)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:94)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:91)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:182)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:205)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:202)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:88)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:196)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:196)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:88)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:86)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:78)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
	at org.apache.spark.sql.Dataset.ofRows(Dataset.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com._4paradigm.openmldb.batch.api.OpenmldbSession.sparksql(OpenmldbSession.scala:209)
	at com._4paradigm.openmldb.batch.utils.HybridseUtil$.hiveLoad(HybridseUtil.scala:378)
	at com._4paradigm.openmldb.batch.utils.HybridseUtil$.autoLoad(HybridseUtil.scala:305)
	at com._4paradigm.openmldb.batch.nodes.LoadDataPlan$.gen(LoadDataPlan.scala:50)
	at com._4paradigm.openmldb.batch.SparkPlanner.visitNode(SparkPlanner.scala:269)
	at com._4paradigm.openmldb.batch.SparkPlanner.getSparkOutput(SparkPlanner.scala:232)
	at com._4paradigm.openmldb.batch.SparkPlanner.$anonfun$plan$1(SparkPlanner.scala:113)
	at com._4paradigm.openmldb.batch.SparkPlanner.withSQLEngine(SparkPlanner.scala:391)
	at com._4paradigm.openmldb.batch.SparkPlanner.plan(SparkPlanner.scala:83)
	at com._4paradigm.openmldb.batch.api.OpenmldbSession.openmldbSql(OpenmldbSession.scala:178)
	at com._4paradigm.openmldb.batch.api.OpenmldbSession.sql(OpenmldbSession.scala:189)
	at com._4paradigm.openmldb.batchjob.util.OpenmldbJobUtil$.runOpenmldbSql(OpenmldbJobUtil.scala:54)
	at com._4paradigm.openmldb.batchjob.ImportOfflineData$.importOfflineData(ImportOfflineData.scala:35)
	at com._4paradigm.openmldb.batchjob.ImportOfflineData$.main(ImportOfflineData.scala:27)
	at com._4paradigm.openmldb.batchjob.ImportOfflineData.main(ImportOfflineData.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
23/05/06 17:13:10 INFO SparkContext: Invoking stop() from shutdown hook
23/05/06 17:13:10 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/05/06 17:13:10 INFO MemoryStore: MemoryStore cleared
23/05/06 17:13:10 INFO BlockManager: BlockManager stopped
23/05/06 17:13:10 INFO BlockManagerMaster: BlockManagerMaster stopped
23/05/06 17:13:10 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/05/06 17:13:10 INFO SparkContext: Successfully stopped SparkContext
23/05/06 17:13:10 INFO ShutdownHookManager: Shutdown hook called
23/05/06 17:13:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-eb45956c-373b-440b-953d-edd72dbe1453
23/05/06 17:13:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-9a6c1e29-946a-4a37-9ba5-9574f4f08dd7
