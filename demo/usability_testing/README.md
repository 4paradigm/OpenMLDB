# Usability testing

## Introduction

This directory contains the black-box testing for the project. The tests are sql scripts that are run against the database. But some of the tests need setup and the setup may can not be done automatically.

Notice that, we need to check the result manually, grep `ERROR` in sql results and check the output result data, **not only check job status**. We may support run scripts by diag tool, and it will check the result automatically(add mark to let diag tool sleep for some async ops).

## Setup data

In some cases, you should setup data before running the tests. The data can be generated by `data_mocker.py` or you can prepare the data manually.
```
pip3 install faker pandas numpy click pyarrow
python3 data_mocker.py -s "create table t1(c1 int, c2 string)" -o test -nf 10 -n 1000
python3 data_mocker.py -s "c1 int, c2 string" -o test -nf 10 -n 1000
```

## simple_test.sql

This directory contains the simple tests. The tests are simple and can be run automatically. No setup needed.

```shell
/work/openmldb/bin/openmldb --zk_cluster=127.0.0.1:2181 --zk_root_path=/openmldb --role=sql_client --interactive=false < simple_test.sql
# or
cat simple_test.sql | /work/openmldb/bin/openmldb --zk_cluster=127.0.0.1:2181 --zk_root_path=/openmldb --role=sql_client --interactive=false
```

If some async jobs still running after drop, you can ignore them, just need to care about sync jobs. If you want check async jobs, you should comment out the `drop table/database` statement in the script. To check the result, you can `SHOW JOBS` or use diag tool to check the result.

So `RUNNING` is ok, but there should be no `ERROR`/`FAILED` in the result.

## offline

This script contains the offline tests. The tests are simple and can be run automatically. But it needs setup, may needs to be done manually.

Setup:

- Generate data, you can gen a big data set to check about spark execute config. Here just a small data set.
```
# generate data, multi files, big file e.g. 1G/512M, to meet executor memory limit
python3 data_mocker.py -s "create table t1(c1 int, c2 string)" -o /tmp/openmldb_test/t1/ -nf 10 -n 1000 -f csv
python3 data_mocker.py -s "create table t1(c1 int, c2 string)" -o /tmp/openmldb_test/t2/ -nf 10 -n 1000
ls /tmp/openmldb_test/t1/*.csv
ls /tmp/openmldb_test/t2/*.parquet
```

- Source place, hdfs/local/hive, choose one.
    - Local spark and file, so just use the date generated in `file:///tmp/openmldb_test`. And dst path is also local.
    ```{note}
    We assume the taskmanager process is running on the same machine as the test, and the spark master is local. Distributed cluster should use hdfs path.
    ```
    ```
    sed "s#<src_path>#file:///tmp/openmldb_test#" offline_test.sql.template > offline_test.sql
    sed -i'' "s#<dst_path>#file:///tmp/openmldb_testout#" offline_test.sql
    ```
    - HDFS, you should put the data to hdfs, and give the hdfs path. e.g. 
    ```
    HDFS_PATH=hdfs://0.0.0.0/tmp # or hdfs:///tmp, hdfs cluster is needless, use the cluster in spark config
    sed "s#<src_path>#${HDFS_PATH}/openmldb_test#" offline_test.sql.template > offline_test.sql
    sed -i'' "s#<dst_path>#${HDFS_PATH}/openmldb_testout#" offline_test.sql
    ```
    ```{note}
    Data in HDFS is `$HDFS_PATH/openmldb_test/t1` and `$HDFS_PATH/openmldb_test/t2`, and the dst path is `$HDFS_PATH/openmldb_testout`. We can create dst dir when `SELECT INTO`, but it needs write permission of `$HDFS_PATH`. 
    ```
    - Hive, you should create hive table t1&t2, load data to hive talbes, and give the hive path with database, e.g. 
    ```
    HIVE_PATH=hive://db1.
    sed "s#<src_path>#${HIVE_PATH}#" offline_test.sql.template > offline_test.sql
    sed -i'' "s#<dst_path>#${HIVE_PATH}#" offline_test.sql
    ```
    ```{note}
    Table in HIVE is `db1.t1` and `db1.t2`, and the dst path is `db1.t1_deep`, `db1.t2_deep`, etc. We can create dst table when `SELECT INTO`, but it needs write permission of HIVE.

    If you want use custom hive table, feel free to modify the script.
    ```

Run(no matter source type):
```
/work/openmldb/bin/openmldb --zk_cluster=127.0.0.1:2181 --zk_root_path=/openmldb --role=sql_client --interactive=false < offline_test.sql
```

Check:

You should check data in dst path, get the right row count, e.g. 10*1000=10000, but csv files have header, so one file has one more line. So you should run:
```
wc -l /tmp/openmldb_testout/t1_deep/*.csv | grep -v total | awk '{sum+=$1-1}END{print sum}'
wc -l /tmp/openmldb_testout/t1_soft/*.csv | grep -v total | awk '{sum+=$1-1}END{print sum}'
wc -l /tmp/openmldb_testout/t2_deep/*.csv | grep -v total | awk '{sum+=$1-1}END{print sum}'
# _double should be 2*total_row_count
wc -l /tmp/openmldb_testout/t1_double/*.csv | grep -v total | awk '{sum+=$1-1}END{print sum}'
wc -l /tmp/openmldb_testout/t2_double/*.csv | grep -v total | awk '{sum+=$1-1}END{print sum}'
```

HDFS could use ```hdfs dfs -cat <path> | wc -l``` to get total rows with header rows, and use `hdfs dfs -ls` to check file count. Or download the files to local and check.

HIVE could use `select count(*) from <db>.<table>` in HIVE Cli to get total rows.

We use offline engine(Spark) to load online data, so online data load is tested here. Check the `Row` in the result of script(`show table status`) to see if the row count is right.

## udf

Setup:
- download libtest_udf.so, src is `src/examples/test_udf.cc`
```
curl -SLO https://openmldb.ai/download/testing/libtest_udf.so
```
- copy libtest_udf.so to the right place
    - demo onebox
    ```shell
    cp libtest_udf.so /tmp/openmldb/tablet-1/udf
    cp libtest_udf.so /tmp/openmldb/tablet-2/udf
    cp libtest_udf.so /work/openmldb/taskmanager/bin/udf
    ```
    - distributed cluster, copy or download to all TabletServer `udf/` and all TaskManager `taskmanager/bin/udf/` manually

Run:
```
/work/openmldb/bin/openmldb --zk_cluster=127.0.0.1:2181 --zk_root_path=/openmldb --role=sql_client --interactive=false < udf_test.sql 
```
